{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swarm_Dst 科研手册\n",
    "\n",
    "## 0. 本机的conda环境选择\n",
    "Swarm_Dst\n",
    "\n",
    "pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. 数据读取操作\n",
    "\n",
    "    对下载到的SWarm数据与Dst数据进行预处理，Swarm数据下载方法见readme.ipynb，此次下载的Swarm数据中存在有卫星提供的Dst值，\n",
    "    同时，也有从地磁台站下载解算得到的Dst值，两者存在差异，此次实验将会对两种Dst数值均进行实验解算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swarm_Dst_finished脚本功能：\n",
    "读取Swarm卫星数据，并进行数据筛选，筛选出特定经纬度范围内，以及特定时间段内的数据\n",
    "1. 经纬度手动调整，参考依据：文献Fast Dst computation by applying deep learning to Swarm satellite magnetic data\n",
    "2. 特定时间段，最安静的10天+最受干扰的5天，参考依据：文献Fast Dst computation by applying deep learning to Swarm satellite magnetic data + \n",
    "   从世界地磁数据中心下载的QDdays数据，格式见网站。\n",
    "3. 输出数据文件保存至Swarm_select.npy，目前仅有SWarm提供的Dst数值，并未加入台站测量的Dst值  !!!结合有关产品手册来看两者的dst值应该是一样的\n",
    "4. ['Spacecraft', 'Timestamp', 'Latitude', 'Longitude', 'Radius', 'B_NEC', 'B_NEC_CHAOS-internal', 'Dst', 'QDLat', 'QDLon'] !!!!getdata.py下载得到的文件中每一行的参数顺序不一样 切记\n",
    "5. !!! 环境选用Swarm_Dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().clear()\n",
    "import cdflib\n",
    "import os\n",
    "import numpy as np\n",
    "import myfunction\n",
    "import logging\n",
    "\n",
    "\n",
    "#----------------------确定筛选参数\n",
    "belt_width_lat = 0.2\n",
    "lat_cen        = [-40,-30,-20,20,30,40]  # 单位：度\n",
    "\n",
    "belt_width_lon = 360.0\n",
    "lon_cen        = [0]\n",
    "\n",
    "Swarm_dir      = \"../Swarm_Data/\"\n",
    "Dst_index      = \"../Dst_index/index.dat\"\n",
    "QDday          = \"../QDday/QDday.txt\"\n",
    "select_dir     = \"../Swarm_select/\"\n",
    "\n",
    "#----------------------读取QDdays文件并保存到相应数据矩阵中\n",
    "\n",
    "QDday_data = myfunction.QDread(QDday)\n",
    "QDday_data = np.delete(QDday_data,range(0,11),axis=0)  # 将QDday数据的时间轴与Swarm数据对齐\n",
    "\n",
    "#--------------------- Create a output logging\n",
    "logger = logging.getLogger('mylogger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger_name = 'my_log.txt'\n",
    "fh = logging.FileHandler(logger_name)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "#-----------------------创建保存输出文件的文件夹\n",
    "if not os.path.exists(select_dir):\n",
    "    os.makedirs(select_dir)\n",
    "\n",
    "\n",
    "#----------------------获取所有数据的文件名\n",
    "files = sorted(os.listdir(Swarm_dir))   # sorted函数用于排序\n",
    "\n",
    "\n",
    "#------------------------文件戳 记录第几个文件\n",
    "file_epoch = 1\n",
    "\n",
    "for file_name in files:  \n",
    "    file_path = Swarm_dir + file_name\n",
    "    cdf_file = cdflib.CDF(file_path)\n",
    "    var = cdf_file.cdf_info()\n",
    "    #----------------------针对每一个文件中的数据进行筛选 \n",
    "    data_mat_finish = np.ones((1,14))   # 初始化数据保存矩阵，对数据文件中的每一行进行筛选判断，若满足条件则加入其中 第一行是无意义的1，后续需要删除\n",
    "\n",
    "    #-----------------------------\n",
    "\n",
    "    raw_data = cdf_file.varget(var.zVariables[0])   #读入数据文件中的所有初始数据\n",
    "    for i in ['Timestamp', 'Latitude', 'Longitude', 'Radius', 'B_NEC', 'B_NEC_CHAOS-internal', 'Dst', 'QDLat', 'QDLon']:\n",
    "        data0 = cdf_file.varget(i)\n",
    "        raw_data = np.column_stack((raw_data,data0))\n",
    "\n",
    "    for i in range(0,raw_data.shape[0]):           # 进行数据筛选\n",
    "        raw_data_test   = raw_data[i,:].reshape(1,-1)\n",
    "        lat             = float(raw_data_test[0,2])\n",
    "        lon             = float(raw_data_test[0,3])\n",
    "        time_           = cdflib.cdfepoch.to_datetime(float(raw_data_test[0,1]))\n",
    "        year            = int(str(time_[0])[0:4])\n",
    "        mon             = int(str(time_[0])[5:7])\n",
    "        day             = int(str(time_[0])[8:10])\n",
    "\n",
    "\n",
    "        QDday_data_test = QDday_data[file_epoch-1,:].reshape(1,-1)\n",
    "        QD_year         = QDday_data_test[0,0]\n",
    "        QD_mon          = QDday_data_test[0,1]\n",
    "        \n",
    "        if (year != QD_year) or (mon != QD_mon) :   # 判断时间轴是否对齐，否则跳出循环\n",
    "            print(f\"{year}-{mon}Time is wrong!!!\")\n",
    "            break\n",
    "        \n",
    "        if not (day in QDday_data_test[0,2:17]):\n",
    "            continue\n",
    "        elif not myfunction.location_pd(lat,belt_width_lat,lat_cen):\n",
    "            continue\n",
    "        elif not myfunction.location_pd(lon,belt_width_lon,lon_cen):\n",
    "            continue\n",
    "        \n",
    "        data_mat_finish = np.row_stack((data_mat_finish,raw_data_test))\n",
    "\n",
    "    log_message    =  file_name+\"has been calculated.\"\n",
    "    logger.debug(log_message)\n",
    "    #-----------------------------------------------\n",
    "    file_epoch += 1\n",
    "        \n",
    "    #---------------------------------------------------save data\n",
    "    data_mat_finish = np.delete(data_mat_finish,0,axis=0)   # 第一行需要删除\n",
    "    save_file       = select_dir + 'Swarm_data_' + str(year).zfill(4) + str(mon).zfill(2) + '_finishedtest.npy'\n",
    "    np.save(save_file,data_mat_finish)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预处理\n",
    "    从卫星观测值中扣除地磁场内部贡献（地核+地壳）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swarm_Dst_process脚本功能：\n",
    "读取筛选后的Swarm卫星数据（/Swarm_select），并进行数据预处理，扣除地磁场内部影响，并输出后续深度学习所需要的数据格式,保存至\"train_valid_database/\"文件夹\n",
    "1. 选择以下7个变量作为网络的输入“特征”：地磁纬度、地磁经度、地磁当地时间(MLat, MLon and MLT)，卫星高度以及三个地磁分量残差(res. X, Y和Z)。参考依据：文献Fast Dst computation \n",
    "   by applying deep learning to Swarm satellite magnetic data\n",
    "2. 筛选出训练集、验证集，本次实验的思路是：使用交叉验证法，分成10组，同时由于Dst的特性，有实时Dst值，临时Dst值和最终Dst值（三者的计算方法和详细定义见Version definition Dst）\n",
    "   尽量保证三种Dst值的训练集和验证集分布相同   ！！！由于Dst会定期更新，截止本次实验，最终Dst值（~2016-12）、临时Dst值（2017-01~2023-06）、实时Dst值（2023-07~）\n",
    "3. 输出数据文件夹\"train_valid_database/\"\n",
    "4. ['Spacecraft', 'Timestamp', 'Latitude', 'Longitude', 'Radius', 'B_NEC', 'B_NEC_CHAOS-internal', 'Dst', 'QDLat', 'QDLon']  !!!!getdata.py下载得到的文件中每一行的参数顺序不一样 切记  在第一步中需要进行调整\n",
    "5. !!! 环境选用Swarm_Dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().clear()\n",
    "import os\n",
    "import cdflib\n",
    "import numpy as np\n",
    "import myfunction\n",
    "import logging\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "#----------------------确定输入参数\n",
    "select_dir             = \"../Swarm_select/\"  # 上一步的输出文件夹\n",
    "groups_num             = 10     # 交叉验证法的总组数\n",
    "\n",
    "final_Dst_begin        = [2013,12,1,0,0,0,0]   # 数字含义：年。月，日，时，分，秒，毫秒\n",
    "final_Dst_end          = [2016,12,31,23,59,59,999]\n",
    "provisional_Dst_begin  = [2017,1,1,0,0,0,0]   # 数字含义：年。月，日，时，分，秒，毫秒\n",
    "provisional_Dst_end    = [2023,6,30,23,59,59,999]\n",
    "real_time_Dst_begin    = [2023,7,1,0,0,0,0]   # 数字含义：年。月，日，时，分，秒，毫秒\n",
    "real_time_Dst_end      = [2023,12,31,23,59,59,999]\n",
    "\n",
    "train_valid_data_dir   = \"../train_valid_database/\"\n",
    "\n",
    "#--------------------- Create a output logging\n",
    "logger = logging.getLogger('mylogger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger_name = 'my_log.txt'\n",
    "fh = logging.FileHandler(logger_name)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "#-----------------------创建保存输出文件的文件夹\n",
    "if not os.path.exists(train_valid_data_dir):\n",
    "    os.makedirs(train_valid_data_dir)\n",
    "\n",
    "#----------------------获取所有数据的文件名\n",
    "files = sorted(os.listdir(select_dir))   # sorted函数用于排序\n",
    "\n",
    "#----------------------读取筛选得到Swarm_select数据，进行汇总与分类.\n",
    "\n",
    "final_mat          = np.zeros((1,14))    # 初始化数据保存矩阵，对数据文件中的每一行进行筛选判断，若满足条件则加入其中 第一行是无意义的0，后续需要删除\n",
    "provisional_mat    = np.zeros((1,14))\n",
    "real_time_mat      = np.zeros((1,14))\n",
    "\n",
    "\n",
    "for file_name in files:\n",
    "    data_matrix = np.load(select_dir+file_name)\n",
    "    time_epoch  = data_matrix[0,1]\n",
    "    if (float(time_epoch) >= cdflib.cdfepoch.compute_epoch(final_Dst_begin)) and (float(time_epoch) <= cdflib.cdfepoch.compute_epoch(final_Dst_end)) :\n",
    "        final_mat           =  np.vstack((final_mat,data_matrix))\n",
    "        continue\n",
    "    elif (float(time_epoch) >= cdflib.cdfepoch.compute_epoch(provisional_Dst_begin)) and (float(time_epoch) <= cdflib.cdfepoch.compute_epoch(provisional_Dst_end)) :\n",
    "        provisional_mat     =  np.vstack((provisional_mat,data_matrix))\n",
    "        continue\n",
    "    elif (float(time_epoch) >= cdflib.cdfepoch.compute_epoch(real_time_Dst_begin)) and (float(time_epoch) <= cdflib.cdfepoch.compute_epoch(real_time_Dst_end)) :\n",
    "        real_time_mat       =  np.vstack((real_time_mat,data_matrix))\n",
    "        continue\n",
    "    \n",
    "    log_message    =  file_name + \" has been calculated.\"\n",
    "    logger.debug(log_message)\n",
    "\n",
    "\n",
    "#---------------------------------------------------save data 分别保存到对应的三类Dst数值中\n",
    "final_mat        = np.delete(final_mat,0,axis=0)   # 第一行需要删除\n",
    "provisional_mat  = np.delete(provisional_mat,0,axis=0)\n",
    "real_time_mat    = np.delete(real_time_mat,0,axis=0)\n",
    "\n",
    "save_file        = train_valid_data_dir + 'final_mat.npy'\n",
    "np.save(save_file,final_mat)    \n",
    "save_file        = train_valid_data_dir + 'provisional_mat.npy'\n",
    "np.save(save_file,provisional_mat)\n",
    "save_file        = train_valid_data_dir + 'real_time_mat.npy'\n",
    "np.save(save_file,real_time_mat) \n",
    "\n",
    "#--------------------------------------------------对三类Dst数值进行分组，打乱顺序后，分为n组，进行交叉验证\n",
    "#--------------------------------------------------在pytorch中可以进行乱序加载，由于在编写此处代码时，尚未知道这个功能，因此额外写了乱序\n",
    "\n",
    "final_index              =  list(range(0,final_mat.shape[0]))\n",
    "random.shuffle(final_index)\n",
    "final_mat[:]             =  final_mat[final_index]\n",
    "\n",
    "provisional_index        =  list(range(0,provisional_mat.shape[0]))\n",
    "random.shuffle(final_index)\n",
    "provisional_mat[:]       =  provisional_mat[provisional_index]\n",
    "\n",
    "real_time_index          =  list(range(0,real_time_mat.shape[0]))\n",
    "random.shuffle(final_index)\n",
    "real_time_mat[:]         =  real_time_mat[real_time_index]\n",
    "\n",
    "final_index_start        = 0         #由于矩阵的行数不一定被n组整除，因此奇数组是向下取整，偶数组向上取整\n",
    "provisional_index_start  = 0\n",
    "real_time_index_start    = 0\n",
    "for i in range(1,(groups_num+1)) :\n",
    "\n",
    "    save_file      = train_valid_data_dir + 'database' + str(i).zfill(2) + '.npy'\n",
    "\n",
    "    if (i % 2 == 1) and (i != groups_num):\n",
    "        database   =  final_mat[final_index_start:(final_index_start + int(final_mat.shape[0]/groups_num)),:]\n",
    "        database   =  np.vstack((database,provisional_mat[provisional_index_start:(provisional_index_start + int(provisional_mat.shape[0]/groups_num)),:]))\n",
    "        database   =  np.vstack((database,real_time_mat[real_time_index_start:(real_time_index_start + int(real_time_mat.shape[0]/groups_num)),:]))\n",
    "        final_index_start        += int(final_mat.shape[0]/groups_num)\n",
    "        provisional_index_start  += int(provisional_mat.shape[0]/groups_num)\n",
    "        real_time_index_start    += int(real_time_mat.shape[0]/groups_num)\n",
    "    elif (i % 2 == 0) and (i != groups_num):\n",
    "        database   =  final_mat[final_index_start:(final_index_start + math.ceil(final_mat.shape[0]/groups_num)),:]\n",
    "        database   =  np.vstack((database,provisional_mat[provisional_index_start:(provisional_index_start + math.ceil(provisional_mat.shape[0]/groups_num)),:]))\n",
    "        database   =  np.vstack((database,real_time_mat[real_time_index_start:(real_time_index_start + math.ceil(real_time_mat.shape[0]/groups_num)),:]))\n",
    "        final_index_start        += math.ceil(final_mat.shape[0]/groups_num)\n",
    "        provisional_index_start  += math.ceil(provisional_mat.shape[0]/groups_num)\n",
    "        real_time_index_start    += math.ceil(real_time_mat.shape[0]/groups_num)\n",
    "    else :\n",
    "        database   =  final_mat[final_index_start:,:]\n",
    "        database   =  np.vstack((database,provisional_mat[provisional_index_start:,:]))\n",
    "        database   =  np.vstack((database,real_time_mat[real_time_index_start:,:]))\n",
    "    \n",
    "    np.save(save_file,database)\n",
    "    log_message    =  'database' + str(i).zfill(2) + \" has been calculated.\"\n",
    "    logger.debug(log_message)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 深度学习框架的搭建\n",
    "\n",
    "    此步骤环境选用pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回归模型架构\n",
    "    创建一个MLP回归模型的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, ReLU, ModuleList, Sequential, Dropout, Softmax, Tanh\n",
    "\n",
    "#-------------------------对类进行继承和重新定义\n",
    "class MLP(torch.nn.Module) :\n",
    "    def __init__(self, input_n, output_n, num_layer, layer_list, dropout=0.5) :\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_n    = input_n\n",
    "        self.outout_n   = output_n\n",
    "        self.num_layer  = num_layer\n",
    "        self.layer_lsit = layer_list\n",
    "\n",
    "        self.input_layer = Sequential(\n",
    "            Linear(input_n, layer_list[0], bias=False),\n",
    "            ReLU()\n",
    "        )\n",
    "        self.hidden_layer = Sequential()\n",
    "\n",
    "        for index in range(num_layer-1) :\n",
    "            self.hidden_layer.extend([Linear(layer_list[index], layer_list[index+1], bias=False), ReLU()])\n",
    "\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "        self.output_layer = Sequential(\n",
    "            Linear(layer_list[-1], output_n, bias=False)\n",
    "            # ReLU()\n",
    "            # Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    #-------------------此处forward函数强烈建议命名为forward，由于魔术方法的存在，会自动调用forward方法，\n",
    "    #-------------------即object.forward(x) 的作用等于 object(x) 但不建议写object.forward(x)\n",
    "    #-------------------因为由于魔术方法的存在，这样写会导致这个方法调用两次，详细机理见收藏的链接\n",
    "    def forward(self, x) :                       \n",
    "        in_put = self.input_layer(x)\n",
    "        hidden = self.hidden_layer(in_put)\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.output_layer(hidden)\n",
    "        output = output.view(-1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度学习相关函数封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "#-------------------------定义一个数据格式的类，继承自torch.utils.data.Dataset\n",
    "class Dataset_(torch.utils.data.Dataset) :\n",
    "    def __init__(self, data_df) :   #-----------此处由于父类并没有init，因此不必使用super关键字\n",
    "        self.label   = torch.from_numpy(data_df['Dst'].values)\n",
    "        self.data    = torch.from_numpy(data_df[data_df.colums[:-1]].values).to(torch.float32)\n",
    "\n",
    "    # --------复写父类的__getitem__\n",
    "    def __getitem__(self, index) :\n",
    "        batch_data  = self.get_batch_data(index)\n",
    "        batch_label = self.get_batch_label(index)\n",
    "        return batch_data, batch_label \n",
    "\n",
    "    def classes(self) :\n",
    "        return self.label\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return self.data.size(0)\n",
    "    \n",
    "    def get_batch_label(self,index) :\n",
    "        return np.array(self.label[index])\n",
    "    \n",
    "    def get_batch_data(self, index) :\n",
    "        return self.data[index]\n",
    "\n",
    "\n",
    "#----------------------------保存数据，加载数据\n",
    "class Config_finished() : #-----------------已经划分好训练集，验证集，测试集\n",
    "    '''\n",
    "    针对本次实验，由于在之前的步骤中，已将数据分为10组\n",
    "    此次可直接用于封装已经划分好训练集，验证集，测试集的数据\n",
    "    '''\n",
    "    def __init__(self, data_dir_path, name, train_list, valid_list, test_list, batch_size, learning_rate, epoch) -> None:\n",
    "        \"\"\"\n",
    "        data_dir_path     : string 数据文件所在文件夹路径\n",
    "        name              : string 模型名字\n",
    "        train_list        : lsit 训练集的编号  7个\n",
    "        valid_list        : list 验证集的编号  1个\n",
    "        test_list         : list 测试集合的编号  2个\n",
    "        batch_size        : int 多少条数据组成一个batch\n",
    "        learning_rate     : float 学习率\n",
    "        epoch             : int 学习轮数\n",
    "        train_loader, valid_loader, test_loader  : 训练数据、验证数据、测试数据\n",
    "        \"\"\"\n",
    "\n",
    "        self.name           = name\n",
    "        self.data_dir_path  = data_dir_path\n",
    "        self.train_list     = train_list\n",
    "        self.valid_list     = valid_list\n",
    "        self.test_list      = test_list\n",
    "        self.batch_size     = batch_size\n",
    "        self.learning_rate  = learning_rate\n",
    "        self.epoch          = epoch\n",
    "        self.train_loader, self.valid_loader, self.test_loader = self.load_tdt()\n",
    "\n",
    "    #------------------将读取的numpy文件转化为dataframe\n",
    "    def transform(self, path_name) :\n",
    "        data_npy   = np.load(path_name)\n",
    "        data_npy   = np.delete(data_npy, 0, axis=1)  # 删除了第一行 也就是Spacecraft\n",
    "        data_npy   = data_npy.astype(np.float32)\n",
    "        df         = pd.DataFrame(data_npy)\n",
    "        Config_finished.if_nan(df)\n",
    "        df.columns = ['Timestamp', 'Latitude', 'Longitude', 'Radius', 'B_N', 'B_E', 'B_C',\n",
    "                    'B_N_CHAOS-internal', 'B_E_CHAOS-internal', 'B_C_CHAOS-internal', 'Dst', 'QDLat', 'QDLon']\n",
    "        df         = df.reindex(columns=['Timestamp', 'Latitude', 'Longitude', 'Radius', 'B_N', 'B_E', 'B_C',\n",
    "                    'B_N_CHAOS-internal', 'B_E_CHAOS-internal', 'B_C_CHAOS-internal', 'QDLat', 'QDLon', 'Dst'])\n",
    "        return df\n",
    "    \n",
    "\n",
    "    #------------------生成数据集集并封装成Dataloader类，需要读入数据集的选项\n",
    "    def dataset_create(self, list) :\n",
    "        '''\n",
    "        list输入你想要生成总数据集的子数据集编号，在此次实验中是1-10\n",
    "        '''\n",
    "        i =  list[0]\n",
    "        path_name           = self.data_dir_path + \"database\" + str(i).zfill(2) + \".npy\"\n",
    "        data_frame_finished = self.transform(path_name) \n",
    "        for i in list[1:] :\n",
    "            path_name           = self.data_dir_path + \"database\" + str(i).zfill(2) + \".npy\"\n",
    "            data_frame          = self.transform(path_name) \n",
    "            data_frame_finished = pd.concat([data_frame_finished, data_frame])\n",
    "\n",
    "        dataset  =  Dataset_(data_frame_finished)\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    #-------------------生成训练集、验证集、测试集\n",
    "    def load_tdt(self) :\n",
    "        train_loader = self.dataset_create(self.train_list)\n",
    "        valid_loader = self.dataset_create(self.valid_list)\n",
    "        test_loader  = self.dataset_create(self.test_list)\n",
    "        return train_loader, valid_loader, test_loader\n",
    "\n",
    "    #-------------------判断数据集是否有空数值 (输入数据格式为pd.dataframe) \n",
    "    #-------------------该方法为静态方法，可以不用通过实例化来使用\n",
    "    @staticmethod\n",
    "    def if_nan(dataframe) :\n",
    "        if dataframe.isnull().any().any():\n",
    "            emp = dataframe.isnull().any()\n",
    "            print(emp[emp].index)\n",
    "            print(\"Empty data exists\")\n",
    "            sys.exit(0) #---------程序正常退出，并进行变量清理等等\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------回归模型的训练、测试和评估\n",
    "class REG_model() :\n",
    "    '''\n",
    "    针对本次实验，对实验数据进行训练、测试和评估等等\n",
    "    并可直接进行可视化操作\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model, config) -> None:\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    def run(self) :\n",
    "        self.train_(self.model)\n",
    "\n",
    "    def train_(self, model) :\n",
    "        dev_best_loss = float('int')\n",
    "        strat_time = time.time()\n",
    "        #-------------------------将模型切换为训练模型\n",
    "        model.train()\n",
    "        #------------------------定义优化器\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.config.learning_rate)\n",
    "        acc_list = [[], []]\n",
    "        loss_list = [[], []]\n",
    "        #-------------------------记录损失不下降的epoch数，到达20之后就直接退出 => 训练无效，再训练下去可能过拟合\n",
    "        break_epoch = 0\n",
    "\n",
    "        for epoch in range(self.config.epoch) :\n",
    "            print('Epoch [{}/{}]'.format(epoch+1,self.config.epoch))\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "path_name = '../train_valid_database/' + 'database01.npy'\n",
    "data_npy   = np.load(path_name)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'colums'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8732\\762745085.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m a = my_utils.Config_finished(data_dir_path='../train_valid_database/',\n\u001b[0m\u001b[0;32m      8\u001b[0m                              \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'reg'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                              \u001b[0mtrain_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                              \u001b[0mvalid_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\Swarm_Dst\\Swarm_Dst_python\\my_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, data_dir_path, name, train_list, valid_list, test_list, batch_size, learning_rate, epoch)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_list\u001b[0m      \u001b[1;33m=\u001b[0m \u001b[0mtest_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m          \u001b[1;33m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_tdt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\python\\Swarm_Dst\\Swarm_Dst_python\\my_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_tdt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtrain_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[0mvalid_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mtest_loader\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\Swarm_Dst\\Swarm_Dst_python\\my_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, list)\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0mpath_name\u001b[0m           \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_dir_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"database\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".npy\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mdata_frame\u001b[0m          \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mdata_frame_finished\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_frame_finished\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mdataset\u001b[0m  \u001b[1;33m=\u001b[0m  \u001b[0mDataset_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_frame_finished\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\Swarm_Dst\\Swarm_Dst_python\\my_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, data_df)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m   \u001b[1;31m#-----------此处由于父类并没有init，因此不必使用super关键字\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Dst'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolums\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\python\\anaconda\\install\\envs\\pytorch\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6292\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6293\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6294\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6296\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'colums'"
     ]
    }
   ],
   "source": [
    "import my_utils\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a = my_utils.Config_finished(data_dir_path='../train_valid_database/',\n",
    "                             name='reg',\n",
    "                             train_list=[1,2,3,4,5,6,7],\n",
    "                             valid_list=[8],\n",
    "                             test_list=[9,10],\n",
    "                             batch_size=10,\n",
    "                             learning_rate=0.1,\n",
    "                             epoch=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下为测试脚本。不可放进主程序运行！！！！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().clear()\n",
    "import MLP_reg\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "rag = MLP_reg.MLP(        \n",
    "        input_n=5,\n",
    "        output_n=1,\n",
    "        num_layer=4,\n",
    "        layer_list=[512, 128, 32, 8],\n",
    "        dropout=0.5)\n",
    "x=torch.tensor([1.0,2.0,3.0,4.0,7.0])\n",
    "#x=torch.tensor([[1.0,2.0,3.0,4.0,7.0],[1.0,2.0,3.0,4.0,6.0]])\n",
    "print(x[-1:])\n",
    "print(x[:-1])\n",
    "\n",
    "a=[1,2,3]\n",
    "\n",
    "print(type(rag.parameters()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().clear()\n",
    "import torch\n",
    "\n",
    "x=torch.tensor([1.0,2.0,3.0,4.0,7.0])\n",
    "y=torch.tensor([1.0,2.0,3.0,4.0,7.0])\n",
    "z=torch.tensor([1.0,2.0,3.0,4.0,7.0],requires_grad=True)\n",
    "m=x+y+z\n",
    "print(m.requires_grad)\n",
    "with torch.no_grad() :\n",
    "    q = m+y+z\n",
    "    print(torch.no_grad().prev)\n",
    "    print(x.requires_grad)\n",
    "    print(m.requires_grad)\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(m.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下为一个简单线性回归的过程实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().clear()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def synthetic_data(w,b,num_examples) :\n",
    "    X = torch.normal(0,1,(num_examples,len(w)))\n",
    "    y = torch.matmul(X,w) + b\n",
    "    y += torch.normal(0,0.01,y.shape)\n",
    "    return X,y.reshape((-1,1))\n",
    "\n",
    "\n",
    "def data_iter(batch_size, features, labels) :\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size) :\n",
    "        batch_indices = torch.tensor(indices[i:min(i + batch_size,num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "\n",
    "\n",
    "def linreg(X, w, b) :\n",
    "    return torch.matmul(X,w) + b\n",
    "\n",
    "def squared_loss(y_hat, y) :\n",
    "    return (y_hat - y.reshape(y_hat.shape)) **2 /2 \n",
    "\n",
    "\n",
    "def sgd(params, lr, batch_size) :\n",
    "    with torch.no_grad() :\n",
    "        for param in params :\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()\n",
    "\n",
    "\n",
    "true_w = torch.tensor([2,-3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w,true_b,1000)\n",
    "\n",
    "batch_size = 10\n",
    "for x, y in data_iter(batch_size, features, labels) :\n",
    "    print(x , '\\n' , y)\n",
    "    break\n",
    "\n",
    "\n",
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "lr = 0.03\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs) :\n",
    "    for X, y in data_iter(batch_size, features, labels) :\n",
    "        l = squared_loss(linreg(X, w, b), y)\n",
    "        l.sum().backward()\n",
    "        sgd([w, b], lr, batch_size)\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        train_l = squared_loss(linreg(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swarm_Dst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
