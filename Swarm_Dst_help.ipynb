{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swarm_Dst 科研手册\n",
    "\n",
    "## 0. 本机的conda环境选择\n",
    "Swarm_Dst\n",
    "\n",
    "pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. 数据读取操作\n",
    "\n",
    "    对下载到的SWarm数据与Dst数据进行预处理，Swarm数据下载方法见readme.ipynb，此次下载的Swarm数据中存在有卫星提供的Dst值，\n",
    "    同时，也有从地磁台站下载解算得到的Dst值，两者存在差异，此次实验将会对两种Dst数值均进行实验解算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swarm_Dst_finished脚本功能：\n",
    "读取Swarm卫星数据，并进行数据筛选，筛选出特定经纬度范围内，以及特定时间段内的数据\n",
    "1. 经纬度手动调整，参考依据：文献Fast Dst computation by applying deep learning to Swarm satellite magnetic data\n",
    "2. 特定时间段，最安静的10天+最受干扰的5天，参考依据：文献Fast Dst computation by applying deep learning to Swarm satellite magnetic data + \n",
    "   从世界地磁数据中心下载的QDdays数据，格式见网站。\n",
    "3. 输出数据文件保存至Swarm_select.npy，目前仅有SWarm提供的Dst数值，并未加入台站测量的Dst值  !!!结合有关产品手册来看两者的dst值应该是一样的\n",
    "4. ['Spacecraft', 'Timestamp', 'Latitude', 'Longitude', 'Radius', 'B_NEC', 'B_NEC_CHAOS-internal', 'Dst', 'QDLat', 'QDLon'] !!!!getdata.py下载得到的文件中每一行的参数顺序不一样 切记\n",
    "5. !!! 环境选用Swarm_Dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().clear()\n",
    "import cdflib\n",
    "import os\n",
    "import numpy as np\n",
    "import myfunction\n",
    "import logging\n",
    "\n",
    "\n",
    "#----------------------确定筛选参数\n",
    "belt_width_lat = 0.2\n",
    "lat_cen        = [-40,-30,-20,20,30,40]  # 单位：度\n",
    "\n",
    "belt_width_lon = 360.0\n",
    "lon_cen        = [0]\n",
    "\n",
    "Swarm_dir      = \"../Swarm_Data/\"\n",
    "Dst_index      = \"../Dst_index/index.dat\"\n",
    "QDday          = \"../QDday/QDday.txt\"\n",
    "select_dir     = \"../Swarm_select/\"\n",
    "\n",
    "#----------------------读取QDdays文件并保存到相应数据矩阵中\n",
    "\n",
    "QDday_data = myfunction.QDread(QDday)\n",
    "QDday_data = np.delete(QDday_data,range(0,11),axis=0)  # 将QDday数据的时间轴与Swarm数据对齐\n",
    "\n",
    "#--------------------- Create a output logging\n",
    "logger = logging.getLogger('mylogger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger_name = 'my_log.txt'\n",
    "fh = logging.FileHandler(logger_name)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "#-----------------------创建保存输出文件的文件夹\n",
    "if not os.path.exists(select_dir):\n",
    "    os.makedirs(select_dir)\n",
    "\n",
    "\n",
    "#----------------------获取所有数据的文件名\n",
    "files = sorted(os.listdir(Swarm_dir))   # sorted函数用于排序\n",
    "\n",
    "\n",
    "#------------------------文件戳 记录第几个文件\n",
    "file_epoch = 1\n",
    "\n",
    "for file_name in files:  \n",
    "    file_path = Swarm_dir + file_name\n",
    "    cdf_file = cdflib.CDF(file_path)\n",
    "    var = cdf_file.cdf_info()\n",
    "    #----------------------针对每一个文件中的数据进行筛选 \n",
    "    data_mat_finish = np.ones((1,14))   # 初始化数据保存矩阵，对数据文件中的每一行进行筛选判断，若满足条件则加入其中 第一行是无意义的1，后续需要删除\n",
    "\n",
    "    #-----------------------------\n",
    "\n",
    "    raw_data = cdf_file.varget(var.zVariables[0])   #读入数据文件中的所有初始数据\n",
    "    for i in ['Timestamp', 'Latitude', 'Longitude', 'Radius', 'B_NEC', 'B_NEC_CHAOS-internal', 'Dst', 'QDLat', 'QDLon']:\n",
    "        data0 = cdf_file.varget(i)\n",
    "        raw_data = np.column_stack((raw_data,data0))\n",
    "\n",
    "    for i in range(0,raw_data.shape[0]):           # 进行数据筛选\n",
    "        raw_data_test   = raw_data[i,:].reshape(1,-1)\n",
    "        lat             = float(raw_data_test[0,2])\n",
    "        lon             = float(raw_data_test[0,3])\n",
    "        time_           = cdflib.cdfepoch.to_datetime(float(raw_data_test[0,1]))\n",
    "        year            = int(str(time_[0])[0:4])\n",
    "        mon             = int(str(time_[0])[5:7])\n",
    "        day             = int(str(time_[0])[8:10])\n",
    "\n",
    "\n",
    "        QDday_data_test = QDday_data[file_epoch-1,:].reshape(1,-1)\n",
    "        QD_year         = QDday_data_test[0,0]\n",
    "        QD_mon          = QDday_data_test[0,1]\n",
    "        \n",
    "        if (year != QD_year) or (mon != QD_mon) :   # 判断时间轴是否对齐，否则跳出循环\n",
    "            print(f\"{year}-{mon}Time is wrong!!!\")\n",
    "            break\n",
    "        \n",
    "        if not (day in QDday_data_test[0,2:17]):\n",
    "            continue\n",
    "        elif not myfunction.location_pd(lat,belt_width_lat,lat_cen):\n",
    "            continue\n",
    "        elif not myfunction.location_pd(lon,belt_width_lon,lon_cen):\n",
    "            continue\n",
    "        \n",
    "        data_mat_finish = np.row_stack((data_mat_finish,raw_data_test))\n",
    "\n",
    "    log_message    =  file_name+\"has been calculated.\"\n",
    "    logger.debug(log_message)\n",
    "    #-----------------------------------------------\n",
    "    file_epoch += 1\n",
    "        \n",
    "    #---------------------------------------------------save data\n",
    "    data_mat_finish = np.delete(data_mat_finish,0,axis=0)   # 第一行需要删除\n",
    "    save_file       = select_dir + 'Swarm_data_' + str(year).zfill(4) + str(mon).zfill(2) + '_finishedtest.npy'\n",
    "    np.save(save_file,data_mat_finish)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预处理\n",
    "    从卫星观测值中扣除地磁场内部贡献（地核+地壳）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swarm_Dst_process脚本功能：\n",
    "读取筛选后的Swarm卫星数据（/Swarm_select），并进行数据预处理，扣除地磁场内部影响，并输出后续深度学习所需要的数据格式,保存至\"train_valid_database/\"文件夹\n",
    "1. 选择以下7个变量作为网络的输入“特征”：地磁纬度、地磁经度、地磁当地时间(MLat, MLon and MLT)，卫星高度以及三个地磁分量残差(res. X, Y和Z)。参考依据：文献Fast Dst computation \n",
    "   by applying deep learning to Swarm satellite magnetic data\n",
    "2. 筛选出训练集、验证集，本次实验的思路是：使用交叉验证法，分成10组，同时由于Dst的特性，有实时Dst值，临时Dst值和最终Dst值（三者的计算方法和详细定义见Version definition Dst）\n",
    "   尽量保证三种Dst值的训练集和验证集分布相同   ！！！由于Dst会定期更新，截止本次实验，最终Dst值（~2016-12）、临时Dst值（2017-01~2023-06）、实时Dst值（2023-07~）\n",
    "3. 输出数据文件夹\"train_valid_database/\"\n",
    "4. ['Spacecraft', 'Timestamp', 'Latitude', 'Longitude', 'Radius', 'B_NEC', 'B_NEC_CHAOS-internal', 'Dst', 'QDLat', 'QDLon']  !!!!getdata.py下载得到的文件中每一行的参数顺序不一样 切记  在第一步中需要进行调整\n",
    "5. !!! 环境选用Swarm_Dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().clear()\n",
    "import os\n",
    "import cdflib\n",
    "import numpy as np\n",
    "import myfunction\n",
    "import logging\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "#----------------------确定输入参数\n",
    "select_dir             = \"../Swarm_select/\"  # 上一步的输出文件夹\n",
    "groups_num             = 10     # 交叉验证法的总组数\n",
    "\n",
    "final_Dst_begin        = [2013,12,1,0,0,0,0]   # 数字含义：年。月，日，时，分，秒，毫秒\n",
    "final_Dst_end          = [2016,12,31,23,59,59,999]\n",
    "provisional_Dst_begin  = [2017,1,1,0,0,0,0]   # 数字含义：年。月，日，时，分，秒，毫秒\n",
    "provisional_Dst_end    = [2023,6,30,23,59,59,999]\n",
    "real_time_Dst_begin    = [2023,7,1,0,0,0,0]   # 数字含义：年。月，日，时，分，秒，毫秒\n",
    "real_time_Dst_end      = [2023,12,31,23,59,59,999]\n",
    "\n",
    "train_valid_data_dir   = \"../train_valid_database/\"\n",
    "\n",
    "#--------------------- Create a output logging\n",
    "logger = logging.getLogger('mylogger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger_name = 'my_log.txt'\n",
    "fh = logging.FileHandler(logger_name)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "#-----------------------创建保存输出文件的文件夹\n",
    "if not os.path.exists(train_valid_data_dir):\n",
    "    os.makedirs(train_valid_data_dir)\n",
    "\n",
    "#----------------------获取所有数据的文件名\n",
    "files = sorted(os.listdir(select_dir))   # sorted函数用于排序\n",
    "\n",
    "#----------------------读取筛选得到Swarm_select数据，进行汇总与分类.\n",
    "\n",
    "final_mat          = np.zeros((1,14))    # 初始化数据保存矩阵，对数据文件中的每一行进行筛选判断，若满足条件则加入其中 第一行是无意义的0，后续需要删除\n",
    "provisional_mat    = np.zeros((1,14))\n",
    "real_time_mat      = np.zeros((1,14))\n",
    "\n",
    "\n",
    "for file_name in files:\n",
    "    data_matrix = np.load(select_dir+file_name)\n",
    "    time_epoch  = data_matrix[0,1]\n",
    "    if (float(time_epoch) >= cdflib.cdfepoch.compute_epoch(final_Dst_begin)) and (float(time_epoch) <= cdflib.cdfepoch.compute_epoch(final_Dst_end)) :\n",
    "        final_mat           =  np.vstack((final_mat,data_matrix))\n",
    "        continue\n",
    "    elif (float(time_epoch) >= cdflib.cdfepoch.compute_epoch(provisional_Dst_begin)) and (float(time_epoch) <= cdflib.cdfepoch.compute_epoch(provisional_Dst_end)) :\n",
    "        provisional_mat     =  np.vstack((provisional_mat,data_matrix))\n",
    "        continue\n",
    "    elif (float(time_epoch) >= cdflib.cdfepoch.compute_epoch(real_time_Dst_begin)) and (float(time_epoch) <= cdflib.cdfepoch.compute_epoch(real_time_Dst_end)) :\n",
    "        real_time_mat       =  np.vstack((real_time_mat,data_matrix))\n",
    "        continue\n",
    "    \n",
    "    log_message    =  file_name + \" has been calculated.\"\n",
    "    logger.debug(log_message)\n",
    "\n",
    "\n",
    "#---------------------------------------------------save data 分别保存到对应的三类Dst数值中\n",
    "final_mat        = np.delete(final_mat,0,axis=0)   # 第一行需要删除\n",
    "provisional_mat  = np.delete(provisional_mat,0,axis=0)\n",
    "real_time_mat    = np.delete(real_time_mat,0,axis=0)\n",
    "\n",
    "save_file        = train_valid_data_dir + 'final_mat.npy'\n",
    "np.save(save_file,final_mat)    \n",
    "save_file        = train_valid_data_dir + 'provisional_mat.npy'\n",
    "np.save(save_file,provisional_mat)\n",
    "save_file        = train_valid_data_dir + 'real_time_mat.npy'\n",
    "np.save(save_file,real_time_mat) \n",
    "\n",
    "#--------------------------------------------------对三类Dst数值进行分组，打乱顺序后，分为n组，进行交叉验证\n",
    "#--------------------------------------------------在pytorch中可以进行乱序加载，由于在编写此处代码时，尚未知道这个功能，因此额外写了乱序\n",
    "\n",
    "final_index              =  list(range(0,final_mat.shape[0]))\n",
    "random.shuffle(final_index)\n",
    "final_mat[:]             =  final_mat[final_index]\n",
    "\n",
    "provisional_index        =  list(range(0,provisional_mat.shape[0]))\n",
    "random.shuffle(final_index)\n",
    "provisional_mat[:]       =  provisional_mat[provisional_index]\n",
    "\n",
    "real_time_index          =  list(range(0,real_time_mat.shape[0]))\n",
    "random.shuffle(final_index)\n",
    "real_time_mat[:]         =  real_time_mat[real_time_index]\n",
    "\n",
    "final_index_start        = 0         #由于矩阵的行数不一定被n组整除，因此奇数组是向下取整，偶数组向上取整\n",
    "provisional_index_start  = 0\n",
    "real_time_index_start    = 0\n",
    "for i in range(1,(groups_num+1)) :\n",
    "\n",
    "    save_file      = train_valid_data_dir + 'database' + str(i).zfill(2) + '.npy'\n",
    "\n",
    "    if (i % 2 == 1) and (i != groups_num):\n",
    "        database   =  final_mat[final_index_start:(final_index_start + int(final_mat.shape[0]/groups_num)),:]\n",
    "        database   =  np.vstack((database,provisional_mat[provisional_index_start:(provisional_index_start + int(provisional_mat.shape[0]/groups_num)),:]))\n",
    "        database   =  np.vstack((database,real_time_mat[real_time_index_start:(real_time_index_start + int(real_time_mat.shape[0]/groups_num)),:]))\n",
    "        final_index_start        += int(final_mat.shape[0]/groups_num)\n",
    "        provisional_index_start  += int(provisional_mat.shape[0]/groups_num)\n",
    "        real_time_index_start    += int(real_time_mat.shape[0]/groups_num)\n",
    "    elif (i % 2 == 0) and (i != groups_num):\n",
    "        database   =  final_mat[final_index_start:(final_index_start + math.ceil(final_mat.shape[0]/groups_num)),:]\n",
    "        database   =  np.vstack((database,provisional_mat[provisional_index_start:(provisional_index_start + math.ceil(provisional_mat.shape[0]/groups_num)),:]))\n",
    "        database   =  np.vstack((database,real_time_mat[real_time_index_start:(real_time_index_start + math.ceil(real_time_mat.shape[0]/groups_num)),:]))\n",
    "        final_index_start        += math.ceil(final_mat.shape[0]/groups_num)\n",
    "        provisional_index_start  += math.ceil(provisional_mat.shape[0]/groups_num)\n",
    "        real_time_index_start    += math.ceil(real_time_mat.shape[0]/groups_num)\n",
    "    else :\n",
    "        database   =  final_mat[final_index_start:,:]\n",
    "        database   =  np.vstack((database,provisional_mat[provisional_index_start:,:]))\n",
    "        database   =  np.vstack((database,real_time_mat[real_time_index_start:,:]))\n",
    "    \n",
    "    np.save(save_file,database)\n",
    "    log_message    =  'database' + str(i).zfill(2) + \" has been calculated.\"\n",
    "    logger.debug(log_message)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 深度学习框架的搭建\n",
    "\n",
    "    此步骤环境选用pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回归模型架构\n",
    "    创建一个MLP回归模型的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, ReLU, ModuleList, Sequential, Dropout, Softmax, Tanh, Sigmoid\n",
    "\n",
    "#-------------------------对类进行继承和重新定义\n",
    "class MLP(torch.nn.Module) :\n",
    "    def __init__(self, input_n, output_n, num_layer, layer_list, dropout=0.5) :\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_n    = input_n\n",
    "        self.outout_n   = output_n\n",
    "        self.num_layer  = num_layer\n",
    "        self.layer_lsit = layer_list\n",
    "\n",
    "        self.input_layer = Sequential(\n",
    "            Linear(input_n, layer_list[0], bias=False),\n",
    "            Sigmoid()\n",
    "        )\n",
    "        self.hidden_layer = Sequential()\n",
    "\n",
    "        for index in range(num_layer-1) :\n",
    "            self.hidden_layer.extend([Linear(layer_list[index], layer_list[index+1], bias=False), Sigmoid()])\n",
    "\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "        self.output_layer = Sequential(\n",
    "            Linear(layer_list[-1], output_n, bias=False)\n",
    "            # ReLU()\n",
    "            # Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    #-------------------此处forward函数强烈建议命名为forward，由于魔术方法的存在，会自动调用forward方法，\n",
    "    #-------------------即object.forward(x) 的作用等于 object(x) 但不建议写object.forward(x)\n",
    "    #-------------------因为由于魔术方法的存在，这样写会导致这个方法调用两次，详细机理见收藏的链接\n",
    "    def forward(self, x) :                       \n",
    "        in_put = self.input_layer(x)\n",
    "        hidden = self.hidden_layer(in_put)\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.output_layer(hidden)\n",
    "        output = output.view(-1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度学习相关函数封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "\n",
    "#-------------------------定义一个数据格式的类，继承自torch.utils.data.Dataset\n",
    "class Dataset_(torch.utils.data.Dataset) :\n",
    "    def __init__(self, data_df) :   #-----------此处由于父类并没有init，因此不必使用super关键字\n",
    "        self.label   = torch.from_numpy(data_df['Dst'].values).to(torch.float)\n",
    "        self.data    = torch.from_numpy(data_df[data_df.columns[:-1]].values).to(torch.float)\n",
    "\n",
    "    # --------复写父类的__getitem__\n",
    "    def __getitem__(self, index) :\n",
    "        batch_data  = self.get_batch_data(index)\n",
    "        batch_label = self.get_batch_label(index)\n",
    "        return   batch_data, batch_label        # 此处必须严格与后面for index,(trains, labels) in enumerate(dataloader)中的括号内容对应\n",
    "                                                # 先特征值，后标签值\n",
    "\n",
    "    def classes(self) :\n",
    "        return self.label\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return self.data.size(0)\n",
    "    \n",
    "    def get_batch_label(self,index) :\n",
    "        return self.label[index]\n",
    "    \n",
    "    def get_batch_data(self, index) :\n",
    "        return self.data[index]\n",
    "\n",
    "\n",
    "#----------------------------保存数据，加载数据\n",
    "class Config_finished() : #-----------------已经划分好训练集，验证集，测试集\n",
    "    '''\n",
    "    针对本次实验，由于在之前的步骤中，已将数据分为10组\n",
    "    此次可直接用于封装已经划分好训练集，验证集，测试集的数据\n",
    "    '''\n",
    "    def __init__(self, data_dir_path, name, train_list, valid_list, test_list, batch_size, learning_rate, epoch) -> None:\n",
    "        \"\"\"\n",
    "        data_dir_path     : string 数据文件所在文件夹路径\n",
    "        name              : string 模型名字\n",
    "        train_list        : lsit 训练集的编号  7个\n",
    "        valid_list        : list 验证集的编号  1个\n",
    "        test_list         : list 测试集合的编号  2个\n",
    "        batch_size        : int 多少条数据组成一个batch\n",
    "        learning_rate     : float 学习率\n",
    "        epoch             : int 输入学习轮数，在输入模型训练后，输出为实际训练的轮数\n",
    "        train_loader, valid_loader, test_loader  : 训练数据、验证数据、测试数据\n",
    "        \"\"\"\n",
    "\n",
    "        self.name           = name\n",
    "        self.data_dir_path  = data_dir_path\n",
    "        self.train_list     = train_list\n",
    "        self.valid_list     = valid_list\n",
    "        self.test_list      = test_list\n",
    "        self.batch_size     = batch_size\n",
    "        self.learning_rate  = learning_rate\n",
    "        self.epoch          = epoch\n",
    "        self.train_loader, self.valid_loader, self.test_loader = self.load_tdt()\n",
    "\n",
    "    #------------------将读取的numpy文件转化为dataframe\n",
    "    def transform(self, path_name) :\n",
    "        data_npy   = np.load(path_name)\n",
    "        data_npy   = np.delete(data_npy, 0, axis=1)  # 删除了第一行 也就是Spacecraft\n",
    "        data_npy   = data_npy.astype(np.float32)\n",
    "        df         = pd.DataFrame(data_npy)\n",
    "        Config_finished.if_nan(df)\n",
    "        df.columns = ['Timestamp', 'Latitude', 'Longitude', 'Radius', 'B_N', 'B_E', 'B_C',\n",
    "                    'B_N_CHAOS-internal', 'B_E_CHAOS-internal', 'B_C_CHAOS-internal', 'Dst', 'QDLat', 'QDLon']\n",
    "        df['B_N_minus'] = df['B_N'] - df['B_N_CHAOS-internal']\n",
    "        df['B_E_minus'] = df['B_E'] - df['B_E_CHAOS-internal']\n",
    "        df['B_C_minus'] = df['B_C'] - df['B_C_CHAOS-internal']\n",
    "        df.drop(['Timestamp','B_N','B_E','B_C','B_N_CHAOS-internal','B_E_CHAOS-internal','B_C_CHAOS-internal',\n",
    "                 'Radius','Latitude', 'Longitude', 'QDLat', 'QDLon'], axis=1, inplace=True)\n",
    "        df         = df.reindex(columns=['B_N_minus', 'B_E_minus', 'B_C_minus', 'Dst'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "\n",
    "    #------------------生成数据集集并封装成Dataloader类，需要读入数据集的选项\n",
    "    def dataset_create(self, list) :\n",
    "        '''\n",
    "        list输入你想要生成总数据集的子数据集编号，在此次实验中是1-10\n",
    "        '''\n",
    "        i =  list[0]\n",
    "        path_name           = self.data_dir_path + \"database\" + str(i).zfill(2) + \".npy\"\n",
    "        data_frame_finished = self.transform(path_name) \n",
    "        for i in list[1:] :\n",
    "            path_name           = self.data_dir_path + \"database\" + str(i).zfill(2) + \".npy\"\n",
    "            data_frame          = self.transform(path_name) \n",
    "            data_frame_finished = pd.concat([data_frame_finished, data_frame])\n",
    "\n",
    "        dataset  =  Dataset_(data_frame_finished)\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    #-------------------生成训练集、验证集、测试集\n",
    "    def load_tdt(self) :\n",
    "        train_loader = self.dataset_create(self.train_list)\n",
    "        valid_loader = self.dataset_create(self.valid_list)\n",
    "        test_loader  = self.dataset_create(self.test_list)\n",
    "        return train_loader, valid_loader, test_loader\n",
    "\n",
    "    #-------------------判断数据集是否有空数值 (输入数据格式为pd.dataframe) \n",
    "    #-------------------该方法为静态方法，可以不用通过实例化来使用\n",
    "    @staticmethod\n",
    "    def if_nan(dataframe) :\n",
    "        if dataframe.isnull().any().any():\n",
    "            emp = dataframe.isnull().any()\n",
    "            print(emp[emp].index)\n",
    "            print(\"Empty data exists\")\n",
    "            sys.exit(0) #---------程序正常退出，并进行变量清理等等\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------回归模型的训练、测试和评估\n",
    "class REG_model() :\n",
    "    '''\n",
    "    针对本次实验，对实验数据进行训练、测试和评估等等\n",
    "    并可直接进行可视化操作\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model, config) -> None:\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    def run(self) :\n",
    "        self.train_(self.model)\n",
    "\n",
    "    def train_(self, model) :\n",
    "        dev_best_loss = float('inf')\n",
    "        strat_time = time.time()\n",
    "        #-------------------------将模型切换为训练模型\n",
    "        model.train()\n",
    "        #------------------------定义优化器\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.config.learning_rate)\n",
    "        acc_list = [[], []]\n",
    "        loss_list = [[], []]\n",
    "        #-------------------------记录损失不下降的epoch数，到达20之后就直接退出 => 训练无效，再训练下去可能过拟合\n",
    "        break_epoch = 0\n",
    "\n",
    "        for epoch in range(self.config.epoch) :\n",
    "            print('Epoch [{}/{}]'.format(epoch+1,self.config.epoch))\n",
    "            for index, (trains, labels) in enumerate(self.config.train_loader) :\n",
    "                # 归零\n",
    "                model.zero_grad()      #---------------进行梯度归零\n",
    "                # 得到预测结果，进行正向解算\n",
    "                outputs = model(trains)\n",
    "                # 计算MSELOSS函数\n",
    "                loss_      = torch.nn.MSELoss() #-----------实例化一个对象\n",
    "                loss_mean  = loss_(outputs, labels)\n",
    "                # 反向传播loss\n",
    "                loss_mean.backward()\n",
    "                # 参数优化与参数更新\n",
    "                optimizer.step()\n",
    "                # 每迭代100次或跑完一个epoch，进行一次验证\n",
    "                if (index % 100 == 0 and index != 0) or index == (len(self.config.train_loader) - 1) :\n",
    "                    true = labels.detach().cpu().numpy()\n",
    "                    # 预测数据\n",
    "                    predict = outputs.detach().cpu().numpy()\n",
    "                    # 计算训练集的准确度 决定系数R2   基于sklearn库 需要转化成nump格式\n",
    "                    # 注意torch库有自带的计算决定系数的函数，输入值与sklearn库有所不同，需要注意\n",
    "                    train_acc = r2_score(true, predict)\n",
    "                    # 计算验证集的准确度 决定系数R2    注意事项同上\n",
    "                    [dev_acc, dev_loss, dev_mse] = self.evaluate(model)\n",
    "                    # 验证loss函数是否进步\n",
    "                    if dev_loss < dev_best_loss :\n",
    "                        dev_best_loss = dev_loss\n",
    "                        improve = '*'\n",
    "                        break_epoch = 0\n",
    "                    else :\n",
    "                        improve = ''\n",
    "                        break_epoch += 1\n",
    "                    # 计算消耗时间\n",
    "                    time_dif = self.get_time_dif(start_time=strat_time)\n",
    "\n",
    "                    # 输出阶段性成果 .item() 方法表示的是  将单元素tensor量 转化为float\n",
    "                    msg = 'Iter:{0:>6},  Train Loss: {1:>5.3},  Train R2: {2:>6.3},  Val Loss: {3:>5.3},  Val R2: {4:>6.3},  Val Mse: {5:>6.3},  Time: {6} {7}'\n",
    "                    print(msg.format(index, loss_mean.item(), train_acc, dev_loss, dev_acc, dev_mse, time_dif, improve))\n",
    "                    # 每当跑完一个epoch，记录画图数据\n",
    "                    if index == (len(self.config.train_loader) - 1) :\n",
    "                        acc_list[0].append(train_acc)\n",
    "                        acc_list[1].append(dev_acc)\n",
    "                        loss_list[0].append(loss_mean.item())\n",
    "                        loss_list[1].append(dev_loss)\n",
    "\n",
    "                    # 转化为训练模式\n",
    "                    model.train()\n",
    "            # 设定早退，防止过拟合，如果20次验证，损失函数没有减小，直接退出训练\n",
    "            if break_epoch > 20 :\n",
    "                self.config.epoch = epoch + 1\n",
    "                break\n",
    "        # 测试\n",
    "        self.test(model)\n",
    "        # 画图 图片默认的保存地址是src文件夹上一级的images文件夹\n",
    "        self.draw_curve(acc_list, loss_list, self.config.epoch)\n",
    "\n",
    "                    \n",
    "    def test(self, model) :\n",
    "        start_time = time.time()\n",
    "        # 测试集准确度R2，损失函数值，MSE\n",
    "        [test_acc, test_loss, test_mse] = self.evaluate(model, test=True)\n",
    "        msg = 'Test R2: {0:>5.3},  Test loss: {1:>6.3},  Test MSE: {2:>6.3}'\n",
    "        print(msg.format(test_acc, test_loss, test_mse))\n",
    "        time_dif = self.get_time_dif(start_time=start_time)\n",
    "        print(\"Time usage:\", time_dif)\n",
    "\n",
    "    \n",
    "    def evaluate(self, model, test=False) :\n",
    "        '''\n",
    "        test=False 使用验证集\n",
    "        test=True  使用测试集\n",
    "        '''\n",
    "        # 转变模型模式\n",
    "        model.eval()\n",
    "        loss_total  = 0\n",
    "        predict_all = np.array([], dtype=float)\n",
    "        labels_all  = np.array([], dtype=float)\n",
    "\n",
    "        if test :\n",
    "            with torch.no_grad() :\n",
    "                for index, (valids, labels) in enumerate(self.config.test_loader) :\n",
    "                    outputs     = model(valids)\n",
    "                    loss_       = torch.nn.MSELoss() #-----------实例化一个对象\n",
    "                    loss_mean   = loss_(outputs, labels)\n",
    "                    loss_total  += loss_mean\n",
    "                    labels      = labels.detach().cpu().numpy()\n",
    "                    predict     = outputs.detach().cpu().numpy()\n",
    "                    labels_all  = np.append(labels_all, labels)\n",
    "                    predict_all = np.append(predict_all, predict)\n",
    "\n",
    "        else :\n",
    "            with torch.no_grad() :\n",
    "                for index, (valids, labels) in enumerate(self.config.valid_loader) :\n",
    "                    outputs     = model(valids)\n",
    "                    loss_       = torch.nn.MSELoss() #-----------实例化一个对象\n",
    "                    loss_mean   = loss_(outputs, labels)\n",
    "                    loss_total  += loss_mean\n",
    "                    labels      = labels.detach().cpu().numpy()\n",
    "                    predict     = outputs.detach().cpu().numpy()\n",
    "                    labels_all  = np.append(labels_all, labels)\n",
    "                    predict_all = np.append(predict_all, predict)\n",
    "\n",
    "        dev_acc = r2_score(labels_all, predict_all)\n",
    "        dev_mse = mean_squared_error(labels_all, predict_all)\n",
    "        #-----------------注意：loss_total / len(self.config.test_loader) ->>>>   dev_loss\n",
    "        #-----------------表示的是损失函数和的均值 \n",
    "        if test :\n",
    "            return dev_acc, loss_total / len(self.config.test_loader), dev_mse\n",
    "        else :\n",
    "            return dev_acc, loss_total / len(self.config.valid_loader), dev_mse\n",
    "\n",
    "\n",
    "    # 计算时间损耗\n",
    "    def get_time_dif(self, start_time) :\n",
    "        end_time = time.time()\n",
    "        time_dif = end_time - start_time\n",
    "        return timedelta(seconds=int(round(time_dif)))\n",
    "    \n",
    "    # 可视化输出\n",
    "    def draw_curve(self, acc_list, loss_list, epochs) :\n",
    "        #-----------------------创建保存输出文件的文件夹\n",
    "        if not os.path.exists('../images/'):\n",
    "            os.makedirs('../images/')\n",
    "        \n",
    "        x = range(0, epochs)\n",
    "        y1 = loss_list[0]\n",
    "        y2 = loss_list[1]\n",
    "        y3 = acc_list[0]\n",
    "        y4 = acc_list[1]\n",
    "        plt.figure(figsize=(13, 13))\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(x, y1, color=\"blue\", label=\"train_loss\", linewidth=2)\n",
    "        plt.plot(x, y2, color=\"orange\", label=\"val_loss\", linewidth=2)\n",
    "        plt.title(\"Loss_curve\", fontsize=20)\n",
    "        plt.xlabel(xlabel=\"Epochs\", fontsize=15)\n",
    "        plt.ylabel(ylabel=\"Loss\", fontsize=15)\n",
    "        plt.legend()\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(x, y3, color=\"blue\", label=\"train_acc\", linewidth=2)\n",
    "        plt.plot(x, y4, color=\"orange\", label=\"val_acc\", linewidth=2)\n",
    "        plt.title(\"Acc_curve\", fontsize=20)\n",
    "        plt.xlabel(xlabel=\"Epochs\", fontsize=15)\n",
    "        plt.ylabel(ylabel=\"Accuracy\", fontsize=15)\n",
    "        plt.legend()\n",
    "        plt.savefig(\"../images/\"+self.config.name+\"_Loss&acc.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MLP_reg\n",
    "import my_utils\n",
    "import gc\n",
    "\n",
    "config_ = my_utils.Config_finished(data_dir_path='../train_valid_database/',\n",
    "                             name='regress',\n",
    "                             train_list=[1,2,3,4,5,6,7],\n",
    "                             valid_list=[8,9],\n",
    "                             test_list=[10],\n",
    "                             batch_size=1000,\n",
    "                             learning_rate=0.01,\n",
    "                             epoch=20)\n",
    "\n",
    "rag = MLP_reg.MLP(        \n",
    "        input_n=3,\n",
    "        output_n=1,\n",
    "        num_layer=3,\n",
    "        layer_list=[7, 14, 7],\n",
    "        dropout=0.0)\n",
    "\n",
    "\n",
    "model = my_utils.REG_model(rag, config_)\n",
    "model.run()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_utils\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a = my_utils.Config_finished(data_dir_path='../train_valid_database/',\n",
    "                             name='reg',\n",
    "                             train_list=[1,2,3,4,5,6,7],\n",
    "                             valid_list=[8],\n",
    "                             test_list=[9,10],\n",
    "                             batch_size=10,\n",
    "                             learning_rate=0.1,\n",
    "                             epoch=10)\n",
    "\n",
    "\n",
    "rag = MLP_reg.MLP(        \n",
    "        input_n=13,\n",
    "        output_n=1,\n",
    "        num_layer=3,\n",
    "        layer_list=[9, 29, 9],\n",
    "        dropout=0.5)\n",
    "\n",
    "for index,trains, labels in enumerate(a.valid_loader):\n",
    "    print(index)\n",
    "    # print(trains)\n",
    "    print(labels)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下为测试脚本。不可放进主程序运行！！！！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().clear()\n",
    "import MLP_reg\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "rag = MLP_reg.MLP(        \n",
    "        input_n=5,\n",
    "        output_n=1,\n",
    "        num_layer=4,\n",
    "        layer_list=[512, 128, 32, 8],\n",
    "        dropout=0.5)\n",
    "x=torch.tensor([1.0,2.0,3.0,4.0,7.0])\n",
    "#x=torch.tensor([[1.0,2.0,3.0,4.0,7.0],[1.0,2.0,3.0,4.0,6.0]])\n",
    "print(x[-1:])\n",
    "print(x[:-1])\n",
    "\n",
    "a=[1,2,3]\n",
    "\n",
    "print(type(rag.parameters()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().clear()\n",
    "import torch\n",
    "\n",
    "x=torch.tensor([1.0,2.0,3.0,4.0,7.0])\n",
    "y=torch.tensor([1.0,2.0,3.0,4.0,7.0])\n",
    "z=torch.tensor([1.0,2.0,3.0,4.0,7.0],requires_grad=True)\n",
    "m=x+y+z\n",
    "print(m.requires_grad)\n",
    "with torch.no_grad() :\n",
    "    q = m+y+z\n",
    "    print(torch.no_grad().prev)\n",
    "    print(x.requires_grad)\n",
    "    print(m.requires_grad)\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(m.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下为一个简单线性回归的过程实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().clear()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def synthetic_data(w,b,num_examples) :\n",
    "    X = torch.normal(0,1,(num_examples,len(w)))\n",
    "    y = torch.matmul(X,w) + b\n",
    "    y += torch.normal(0,0.01,y.shape)\n",
    "    return X,y.reshape((-1,1))\n",
    "\n",
    "\n",
    "def data_iter(batch_size, features, labels) :\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size) :\n",
    "        batch_indices = torch.tensor(indices[i:min(i + batch_size,num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "\n",
    "\n",
    "def linreg(X, w, b) :\n",
    "    return torch.matmul(X,w) + b\n",
    "\n",
    "def squared_loss(y_hat, y) :\n",
    "    return (y_hat - y.reshape(y_hat.shape)) **2 /2 \n",
    "\n",
    "\n",
    "def sgd(params, lr, batch_size) :\n",
    "    with torch.no_grad() :\n",
    "        for param in params :\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()\n",
    "\n",
    "\n",
    "true_w = torch.tensor([2,-3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w,true_b,1000)\n",
    "\n",
    "batch_size = 10\n",
    "for x, y in data_iter(batch_size, features, labels) :\n",
    "    print(x , '\\n' , y)\n",
    "    break\n",
    "\n",
    "\n",
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "lr = 0.03\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs) :\n",
    "    for X, y in data_iter(batch_size, features, labels) :\n",
    "        l = squared_loss(linreg(X, w, b), y)\n",
    "        l.sum().backward()\n",
    "        sgd([w, b], lr, batch_size)\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        train_l = squared_loss(linreg(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swarm_Dst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
