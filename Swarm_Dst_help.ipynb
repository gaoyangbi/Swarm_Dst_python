{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swarm_Dst 科研手册\n",
    "\n",
    "## 0. 本机的conda环境选择\n",
    "Swarm_Dst\n",
    "\n",
    "pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. 数据读取操作\n",
    "\n",
    "    对下载到的SWarm数据与Dst数据进行预处理，Swarm数据下载方法见readme.ipynb，此次下载的Swarm数据中存在有卫星提供的Dst值，\n",
    "    同时，也有从地磁台站下载解算得到的Dst值，两者存在差异，此次实验将会对两种Dst数值均进行实验解算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swarm_Dst_finished脚本功能：\n",
    "读取Swarm卫星数据，并进行数据筛选，筛选出特定经纬度范围内，以及特定时间段内的数据\n",
    "1. 经纬度手动调整，参考依据：文献Fast Dst computation by applying deep learning to Swarm satellite magnetic data\n",
    "2. 特定时间段，最安静的10天+最受干扰的5天，参考依据：文献Fast Dst computation by applying deep learning to Swarm satellite magnetic data + \n",
    "   从世界地磁数据中心下载的QDdays数据，格式见网站。\n",
    "3. 输出数据文件保存至Swarm_select.npy，目前仅有SWarm提供的Dst数值，并未加入台站测量的Dst值  !!!结合有关产品手册来看两者的dst值应该是一样的\n",
    "4. 'Spacecraft' 'Timestamp', 'Latitude', 'Longitude', 'Radius', 'Dst', 'B_NEC_CHAOS-internal', 'QDLon', 'QDLat', 'B_NEC'\n",
    "5. !!! 环境选用Swarm_Dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().clear()\n",
    "import cdflib\n",
    "import os\n",
    "from matplotlib.dates import YearLocator\n",
    "import numpy as np\n",
    "import myfunction\n",
    "import logging\n",
    "\n",
    "\n",
    "#----------------------确定筛选参数\n",
    "belt_width_lat = 0.2\n",
    "lat_cen        = [-40,-30,-20,20,30,40]  # 单位：度\n",
    "\n",
    "belt_width_lon = 360.0\n",
    "lon_cen        = [0]\n",
    "\n",
    "Swarm_dir      = \"Swarm_Data/\"\n",
    "Dst_index      = \"Dst_index/index.dat\"\n",
    "QDday          = \"QDday/QDday.txt\"\n",
    "select_dir     = \"Swarm_select/\"\n",
    "\n",
    "#----------------------读取QDdays文件并保存到相应数据矩阵中\n",
    "\n",
    "QDday_data = myfunction.QDread(QDday)\n",
    "QDday_data = np.delete(QDday_data,range(0,11),axis=0)  # 将QDday数据的时间轴与Swarm数据对齐\n",
    "\n",
    "#--------------------- Create a output logging\n",
    "logger = logging.getLogger('mylogger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger_name = 'my_log.txt'\n",
    "fh = logging.FileHandler(logger_name)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "#-----------------------创建保存输出文件的文件夹\n",
    "if not os.path.exists(select_dir):\n",
    "    os.makedirs(select_dir)\n",
    "\n",
    "\n",
    "#----------------------获取所有数据的文件名\n",
    "files = sorted(os.listdir(Swarm_dir))   # sorted函数用于排序\n",
    "\n",
    "\n",
    "#------------------------文件戳 记录第几个文件\n",
    "file_epoch = 1\n",
    "\n",
    "for file_name in files:  \n",
    "    file_path = Swarm_dir + file_name\n",
    "    cdf_file = cdflib.CDF(file_path)\n",
    "    var = cdf_file.cdf_info()\n",
    "    #----------------------针对每一个文件中的数据进行筛选 \n",
    "    data_mat_finish = np.ones((1,14))   # 初始化数据保存矩阵，对数据文件中的每一行进行筛选判断，若满足条件则加入其中 第一行是无意义的1，后续需要删除\n",
    "\n",
    "    #-----------------------------\n",
    "\n",
    "    raw_data = cdf_file.varget(var.zVariables[0])   #读入数据文件中的所有初始数据\n",
    "    for i in var.zVariables[1:]:\n",
    "        data0 = cdf_file.varget(i)\n",
    "        raw_data = np.column_stack((raw_data,data0))\n",
    "\n",
    "    for i in range(0,raw_data.shape[0]):           # 进行数据筛选\n",
    "        raw_data_test   = raw_data[i,:].reshape(1,-1)\n",
    "        lat             = float(raw_data_test[0,2])\n",
    "        lon             = float(raw_data_test[0,3])\n",
    "        time_           = cdflib.cdfepoch.to_datetime(float(raw_data_test[0,1]))\n",
    "        year            = int(str(time_[0])[0:4])\n",
    "        mon             = int(str(time_[0])[5:7])\n",
    "        day             = int(str(time_[0])[8:10])\n",
    "\n",
    "\n",
    "        QDday_data_test = QDday_data[file_epoch-1,:].reshape(1,-1)\n",
    "        QD_year         = QDday_data_test[0,0]\n",
    "        QD_mon          = QDday_data_test[0,1]\n",
    "        \n",
    "        if (year != QD_year) or (mon != QD_mon) :   # 判断时间轴是否对齐，否则跳出循环\n",
    "            print(f\"{year}-{mon}Time is wrong!!!\")\n",
    "            break\n",
    "        \n",
    "        if not (day in QDday_data_test[0,2:17]):\n",
    "            continue\n",
    "        elif not myfunction.location_pd(lat,belt_width_lat,lat_cen):\n",
    "            continue\n",
    "        elif not myfunction.location_pd(lon,belt_width_lon,lon_cen):\n",
    "            continue\n",
    "        \n",
    "        data_mat_finish = np.row_stack((data_mat_finish,raw_data_test))\n",
    "\n",
    "    log_message    =  file_name+\"has been calculated.\"\n",
    "    logger.debug(log_message)\n",
    "    #-----------------------------------------------\n",
    "    file_epoch += 1\n",
    "        \n",
    "    #---------------------------------------------------save data\n",
    "    data_mat_finish = np.delete(data_mat_finish,0,axis=0)   # 第一行需要删除\n",
    "    save_file       = select_dir + 'Swarm_data_' + str(year).zfill(4) + str(mon).zfill(2) + '_finishedtest.npy'\n",
    "    np.save(save_file,data_mat_finish)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预处理\n",
    "    从卫星观测值中扣除地磁场内部贡献（地核+地壳）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swarm_Dst_process脚本功能：\n",
    "读取筛选后的Swarm卫星数据（/Swarm_select），并进行数据预处理，扣除地磁场内部影响，并输出后续深度学习所需要的数据格式,保存至\"train_valid_database/\"文件夹\n",
    "1. 选择以下7个变量作为网络的输入“特征”：地磁纬度、地磁经度、地磁当地时间(MLat, MLon and MLT)，卫星高度以及三个地磁分量残差(res. X, Y和Z)。参考依据：文献Fast Dst computation \n",
    "   by applying deep learning to Swarm satellite magnetic data\n",
    "2. 筛选出训练集、验证集，本次实验的思路是：使用交叉验证法，分成10组，同时由于Dst的特性，有实时Dst值，临时Dst值和最终Dst值（三者的计算方法和详细定义见Version definition Dst）\n",
    "   尽量保证三种Dst值的训练集和验证集分布相同   ！！！由于Dst会定期更新，截止本次实验，最终Dst值（~2016-12）、临时Dst值（2017-01~2023-06）、实时Dst值（2023-07~）\n",
    "3. 输出数据文件夹\"train_valid_database/\"\n",
    "4. 'Spacecraft' 'Timestamp', 'Latitude', 'Longitude', 'Radius', 'Dst', 'B_NEC_CHAOS-internal', 'QDLon', 'QDLat', 'B_NEC'\n",
    "5. !!! 环境选用Swarm_Dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().clear()\n",
    "import os\n",
    "import cdflib\n",
    "import numpy as np\n",
    "import myfunction\n",
    "import logging\n",
    "\n",
    "\n",
    "#----------------------确定输入参数\n",
    "select_dir             = \"Swarm_select/\"\n",
    "groups_num             = 10     # 交叉验证法的总组数\n",
    "\n",
    "final_Dst_begin        = [2013,12,1,0,0,0,0]   # 数字含义：年。月，日，时，分，秒，毫秒\n",
    "final_Dst_end          = [2016,12,31,23,59,59,999]\n",
    "provisional_Dst_begin  = [2017,1,1,0,0,0,0]   # 数字含义：年。月，日，时，分，秒，毫秒\n",
    "provisional_Dst_end    = [2023,6,30,23,59,59,999]\n",
    "real_time_Dst_begin    = [2023,7,1,0,0,0,0]   # 数字含义：年。月，日，时，分，秒，毫秒\n",
    "real_time_Dst_end      = [2023,12,31,23,59,59,999]\n",
    "\n",
    "train_valid_data_dir   = \"train_valid_database/\"\n",
    "\n",
    "#--------------------- Create a output logging\n",
    "logger = logging.getLogger('mylogger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger_name = 'my_log.txt'\n",
    "fh = logging.FileHandler(logger_name)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "#-----------------------创建保存输出文件的文件夹\n",
    "if not os.path.exists(train_valid_data_dir):\n",
    "    os.makedirs(train_valid_data_dir)\n",
    "\n",
    "#----------------------获取所有数据的文件名\n",
    "files = sorted(os.listdir(select_dir))   # sorted函数用于排序\n",
    "\n",
    "#----------------------读取筛选得到Swarm_select数据，进行汇总与分类.\n",
    "\n",
    "final_mat          = np.zeros((1,14))    # 初始化数据保存矩阵，对数据文件中的每一行进行筛选判断，若满足条件则加入其中 第一行是无意义的0，后续需要删除\n",
    "provisional_mat    = np.zeros((1,14))\n",
    "real_time_mat      = np.zeros((1,14))\n",
    "\n",
    "\n",
    "for file_name in files:\n",
    "    data_matrix = np.load(select_dir+file_name)\n",
    "    time_epoch  = data_matrix[0,1]\n",
    "    if (float(time_epoch) >= cdflib.cdfepoch.compute_epoch(final_Dst_begin)) and (float(time_epoch) <= cdflib.cdfepoch.compute_epoch(final_Dst_end)) :\n",
    "        final_mat           =  np.vstack((final_mat,data_matrix))\n",
    "        continue\n",
    "    elif (float(time_epoch) >= cdflib.cdfepoch.compute_epoch(provisional_Dst_begin)) and (float(time_epoch) <= cdflib.cdfepoch.compute_epoch(provisional_Dst_end)) :\n",
    "        provisional_mat     =  np.vstack((provisional_mat,data_matrix))\n",
    "        continue\n",
    "    elif (float(time_epoch) >= cdflib.cdfepoch.compute_epoch(real_time_Dst_begin)) and (float(time_epoch) <= cdflib.cdfepoch.compute_epoch(real_time_Dst_end)) :\n",
    "        real_time_mat       =  np.vstack((real_time_mat,data_matrix))\n",
    "        continue\n",
    "    \n",
    "    log_message    =  file_name+\"has been calculated.\"\n",
    "    logger.debug(log_message)\n",
    "\n",
    "\n",
    "#---------------------------------------------------save data\n",
    "final_mat        = np.delete(final_mat,0,axis=0)   # 第一行需要删除\n",
    "provisional_mat  = np.delete(provisional_mat,0,axis=0)\n",
    "real_time_mat    = np.delete(real_time_mat,0,axis=0)\n",
    "\n",
    "save_file        = train_valid_data_dir + 'final_mat.npy'\n",
    "np.save(save_file,final_mat)    \n",
    "save_file        = train_valid_data_dir + 'provisional_mat.npy'\n",
    "np.save(save_file,provisional_mat)\n",
    "save_file        = train_valid_data_dir + 'real_time_mat.npy'\n",
    "np.save(save_file,real_time_mat) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 深度学习框架的搭建\n",
    "\n",
    "    此步骤环境选用pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下为测试脚本。不可放进主程序运行！！！！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "print(multiprocessing.cpu_count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swarm_Dst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
